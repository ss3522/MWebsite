{
  "articles": [
    {
      "path": "about.html",
      "title": "Sangbeom Seo",
      "description": "I am from Daegu, South Korea. I like soccer and music and love to talk about them. My favorite sports team is Liverpool FC and Daegu FC!",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\n",
      "last_modified": "2021-12-10T17:30:21+09:00"
    },
    {
      "path": "index.html",
      "title": "Sangbeom Seo",
      "author": [],
      "contents": "\r\n\r\n          \r\n          \r\n          Sangbeom\r\n          \r\n          \r\n          Home\r\n          About\r\n          Resume\r\n          \r\n          \r\n          Project\r\n           \r\n          ▾\r\n          \r\n          \r\n          R-Squared\r\n          Machine Learning\r\n          \r\n          \r\n          ☰\r\n          \r\n          \r\n      \r\n        \r\n          \r\n            Sangbeom Seo\r\n          \r\n          \r\n            \r\n              I am graduate student at Mississippi State University majoring in accounting. Thank you for visiting!\r\n            \r\n            \r\n              I am graduate student at Mississippi State University majoring in accounting. Thank you for visiting!\r\n            \r\n          \r\n\r\n          \r\n            \r\n              \r\n                  \r\n                    \r\n                      LinkedIn\r\n                    \r\n                  \r\n                \r\n                                \r\n                  \r\n                    \r\n                      GitHub\r\n                    \r\n                  \r\n                \r\n                                \r\n                  \r\n                    \r\n                      Instagram\r\n                    \r\n                  \r\n                \r\n                              \r\n          \r\n\r\n          \r\n            \r\n              \r\n                                \r\n                  \r\n                    LinkedIn\r\n                  \r\n                \r\n                                \r\n                  \r\n                    GitHub\r\n                  \r\n                \r\n                                \r\n                  \r\n                    Instagram\r\n                  \r\n                \r\n                              \r\n            \r\n          \r\n        \r\n      \r\n    \r\n\r\n    \r\n    \r\n    ",
      "last_modified": "2021-12-10T17:30:22+09:00"
    },
    {
      "path": "Resume.html",
      "title": "Sangbeom Seo",
      "author": [],
      "contents": "\r\nEDUCATION\r\nMississippi State University, Starkville, MS\r\nCollege of Business - Bachelor of Business Administration (January 2015 - May 2020)\r\nRicard C. Adkerson School of Accountancy - Master of Professional Accounting\r\nCAREER\r\nREPUBLIC OF KOREA AIRFORCE\r\nEnlisted Soldier (September 2015 - September 2017)\r\nKOREAN STUDENT ASSOCIATION\r\nTreasurer (December 2019 - May 2021)\r\nPresident (August 2021)\r\nACADEMIC INTERESTS\r\nFinancial Accounting\r\nAudit\r\nAccounting Data Analytics\r\nRELATED SKILLS\r\nLearned Basic Codes of R, JAVA\r\nFluent in Microsoft Word, Powerpoint, Excel\r\nLanguage; English, Korean\r\nCONTACT\r\n662-312-6933\r\nss3522@msstate.edu\r\n\r\n\r\n\r\n",
      "last_modified": "2021-12-10T17:30:23+09:00"
    },
    {
      "path": "Rsquared.html",
      "title": "Rsquared Bad?",
      "description": "Thank you Dr. Hunt!\n",
      "author": [],
      "contents": "\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nSangbeom\r\n\r\n\r\nHome\r\nAbout\r\nResume\r\n\r\n\r\nProject\r\n \r\n▾\r\n\r\n\r\nR-Squared\r\nMachine Learning\r\n\r\n\r\n☰\r\n\r\n\r\n  \r\n    \r\n      \r\n        \r\n        \r\n        \r\n      \r\n      \r\n    \r\n    \r\n      \r\n  Home\r\n\r\n\r\n  About\r\n\r\n\r\n  Resume\r\n\r\n\r\n  \r\n    Project\r\n     \r\n    \r\n  \r\n  \r\n      R-Squared\r\n    \r\n    \r\n      Machine Learning\r\n    \r\n  \r\n      \r\n  \r\n\r\n\r\n\r\n\r\n\r\n\r\nRsquared Bad?\r\n\r\n\r\n\r\n\r\n\r\nTime to blow RSquared up\r\nR-squared is a statistic that often accompanies regression output. It ranges in value from 0 to 1 and is usually interpreted as summarizing the percent of variation in the response that the regression model explains. So an R-squared of 0.65 might mean that the model explains about 65% of the variation in our dependent variable. Given this logic, we prefer our regression models have a high R-squared.\r\nIn R, we typically get R-squared by calling the summary function on a model object. Here’s a quick example using simulated data:\r\n# independent variable\r\nx <- 1:20 \r\n# for reproducibility\r\nset.seed(1) \r\n# dependent variable; function of x with random error\r\ny <- 2 + 0.5*x + rnorm(20,0,3) \r\n# simple linear regression\r\nmod <- lm(y~x)\r\n# request just the r-squared value\r\nsummary(mod)$r.squared          \r\n## [1] 0.6026682\r\nOne way to express R-squared is as the sum of squared fitted-value deviations divided by the sum of squared original-value deviations:\r\n\\[\r\nR^{2} =  \\frac{\\sum (\\hat{y} – \\bar{\\hat{y}})^{2}}{\\sum (y – \\bar{y})^{2}}\r\n\\]\r\nWe can calculate it directly using our model object like so:\r\n# extract fitted (or predicted) values from model\r\nf <- mod$fitted.values\r\n# sum of squared fitted-value deviations\r\nmss <- sum((f - mean(f))^2)\r\n# sum of squared original-value deviations\r\ntss <- sum((y - mean(y))^2)\r\n# r-squared\r\nmss/tss                      \r\n## [1] 0.6026682\r\n1. R-squared does not measure goodness of fit. It can be arbitrarily low when the model is completely correct. By making\\(σ^2\\) large, we drive R-squared towards 0, even when every assumption of the simple linear regression model is correct in every particular.\r\nWhat is \\(σ^2\\)? When we perform linear regression, we assume our model almost predicts our dependent variable. The difference between “almost” and “exact” is assumed to be a draw from a Normal distribution with mean 0 and some variance we call \\(σ^2\\).\r\nThis statement is easy enough to demonstrate. The way we do it here is to create a function that (1) generates data meeting the assumptions of simple linear regression (independent observations, normally distributed errors with constant variance), (2) fits a simple linear model to the data, and (3) reports the R-squared. Notice the only parameter for sake of simplicity is sigma. We then “apply” this function to a series of increasing \\(σ\\) values and plot the results.\r\nr2.0 <- function(sig){\r\n  # our predictor\r\n  x <- seq(1,10,length.out = 100)   \r\n  # our response; a function of x plus some random noise\r\n  y <- 2 + 1.2*x + rnorm(100,0,sd = sig) \r\n  # print the R-squared value\r\n  summary(lm(y ~ x))$r.squared          \r\n}\r\nsigmas <- seq(0.5,20,length.out = 20)\r\n # apply our function to a series of sigma values\r\nrout <- sapply(sigmas, r2.0)            \r\nplot(rout ~ sigmas, type=\"b\")\r\n\r\nR-squared tanks hard with increasing sigma, even though the model is completely correct in every respect.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n// add bootstrap table styles to pandoc tables\r\nfunction bootstrapStylePandocTables() {\r\n  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');\r\n}\r\n$(document).ready(function () {\r\n  bootstrapStylePandocTables();\r\n});\r\n\r\n\r\n\r\n$(document).ready(function () {\r\n  window.buildTabsets(\"TOC\");\r\n});\r\n\r\n$(document).ready(function () {\r\n  $('.tabset-dropdown > .nav-tabs > li').click(function () {\r\n    $(this).parent().toggleClass('nav-tabs-open');\r\n  });\r\n});\r\n\r\n  (function () {\r\n    var script = document.createElement(\"script\");\r\n    script.type = \"text/javascript\";\r\n    script.src  = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\";\r\n    document.getElementsByTagName(\"head\")[0].appendChild(script);\r\n  })();\r\n",
      "last_modified": "2021-12-10T17:30:24+09:00"
    },
    {
      "path": "SVMandBootstrap.html",
      "title": "SVM and Bagging & RF",
      "description": "Support Vector Machine, Bagging and Random Forest Exercise \n",
      "author": [],
      "contents": "\r\n\r\nContents\r\nExercise 1 (Bootstrap)\r\n1. Redo the bootstrap analysis, but use 10 samples. Change times = 100 to times = 10. How does the population estimate for mean change?\r\n2. Redo the bootstrap analysis, but find the median. What is the difference in population estimate for the median vs mean?\r\n\r\nExercise 1 (SVM)\r\n1. Change the training to testing split size, for example change from a 60%:40% to a 75%:25%, and to a 50%:50%. Compare the results to the 60/40.\r\n2. Change the Kernel…use svmPoly for the method. method = “svmLinear”, to method = “svmPoly”, compare the linear results with the polynomial results.\r\n\r\n\r\nExercise 1 (Bootstrap)\r\n1. Redo the bootstrap analysis, but use 10 samples. Change times = 100 to times = 10. How does the population estimate for mean change?\r\n- The population estimate does not change after changing the number of samples.\r\n\r\n\r\nShow code\r\n\r\nlibrary(curl)\r\nload(curl(\"https://raw.githubusercontent.com/Professor-Hunt/ACC8143/main/data/tips.rda\"))\r\n\r\nset.seed(0)\r\nlibrary(rsample)\r\nlibrary(tidyverse)\r\n\r\n#perform bootstrapping with 2000 replications\r\nresample2 <- bootstraps(as.data.frame(tips$size), times = 10)\r\n\r\n#mean\r\nmean(resample2$splits[[1]]$data$`tips$size`)\r\n\r\n\r\n[1] 2.569672\r\n\r\nShow code\r\n\r\n#standard deviation\r\nsd(resample2$splits[[1]]$data$`tips$size`)\r\n\r\n\r\n[1] 0.9510998\r\n\r\n2. Redo the bootstrap analysis, but find the median. What is the difference in population estimate for the median vs mean?\r\n- Population estimate for median and mean is different. Median is 2 while mean is 2.569672.\r\n\r\n\r\nShow code\r\n\r\nresample1 <- bootstraps(as.data.frame(tips$size), times = 100)\r\nmedian(resample1$splits[[1]]$data$`tips$size`)\r\n\r\n\r\n[1] 2\r\n\r\nShow code\r\n\r\nmean(resample1$splits[[1]]$data$`tips$size`)\r\n\r\n\r\n[1] 2.569672\r\n\r\nExercise 1 (SVM)\r\n1. Change the training to testing split size, for example change from a 60%:40% to a 75%:25%, and to a 50%:50%. Compare the results to the 60/40.\r\n- Compared to the resuls to the 60:40, accuracy and kappa were bigger when testing size was 75:25 and 50:50. Also, they were biggest when the size of testing sample was 50%.\r\n\r\n\r\nShow code\r\n\r\nlibrary(caret)\r\nlibrary(tidyverse)\r\n#set the seed :)\r\nset.seed(1)\r\ntrainIndex <- createDataPartition(iris$Species, p = .75, list = FALSE, times = 1)\r\n\r\n#grab the data\r\nSVMTrain <- iris[ trainIndex,]\r\nSVMTest  <- iris[-trainIndex,]\r\n\r\niris_SVM <- train(\r\n  form = factor(Species) ~ .,\r\n  data = SVMTrain,\r\n  #here we add classProbs because we want probs\r\n  trControl = trainControl(method = \"cv\", number = 10,\r\n                           classProbs =  TRUE),\r\n  method = \"svmLinear\",\r\n  preProcess = c(\"center\", \"scale\"),\r\n  tuneLength = 10)\r\n\r\niris_SVM\r\n\r\n\r\nSupport Vector Machines with Linear Kernel \r\n\r\n114 samples\r\n  4 predictor\r\n  3 classes: 'setosa', 'versicolor', 'virginica' \r\n\r\nPre-processing: centered (4), scaled (4) \r\nResampling: Cross-Validated (10 fold) \r\nSummary of sample sizes: 102, 102, 103, 103, 104, 102, ... \r\nResampling results:\r\n\r\n  Accuracy   Kappa\r\n  0.9833333  0.975\r\n\r\nTuning parameter 'C' was held constant at a value of 1\r\n\r\n\r\n\r\nShow code\r\n\r\ntrainIndex50 <- createDataPartition(iris$Species, p = .5, list = FALSE, times = 1)\r\n\r\n#look at the first few\r\n#head(trainIndex)\r\n\r\n#grab the data\r\nSVMTrain50 <- iris[ trainIndex50,]\r\nSVMTest50  <- iris[-trainIndex50,]\r\n\r\n\r\niris_SVM50 <- train(\r\n  form = factor(Species) ~ .,\r\n  data = SVMTrain50,\r\n  #here we add classProbs because we want probs\r\n  trControl = trainControl(method = \"cv\", number = 10,\r\n                           classProbs =  TRUE),\r\n  method = \"svmLinear\",\r\n  preProcess = c(\"center\", \"scale\"),\r\n  tuneLength = 10)\r\n\r\niris_SVM50\r\n\r\n\r\nSupport Vector Machines with Linear Kernel \r\n\r\n75 samples\r\n 4 predictor\r\n 3 classes: 'setosa', 'versicolor', 'virginica' \r\n\r\nPre-processing: centered (4), scaled (4) \r\nResampling: Cross-Validated (10 fold) \r\nSummary of sample sizes: 69, 67, 68, 67, 69, 66, ... \r\nResampling results:\r\n\r\n  Accuracy   Kappa    \r\n  0.9440476  0.9147671\r\n\r\nTuning parameter 'C' was held constant at a value of 1\r\n\r\n2. Change the Kernel…use svmPoly for the method. method = “svmLinear”, to method = “svmPoly”, compare the linear results with the polynomial results.\r\n\r\n\r\nShow code\r\n\r\nset.seed(1)\r\n#get our samples\r\n#using the iris data\r\n#lets split the data 60/40\r\n\r\ntrainIndex22 <- createDataPartition(iris$Species, p = .6, list = FALSE, times = 1)\r\n\r\n#look at the first few\r\n#head(trainIndex)\r\n\r\n#grab the datas\r\nSVMTrain22 <- iris[ trainIndex22,]\r\nSVMTest22  <- iris[-trainIndex22,]\r\n\r\n\r\niris_SVM22 <- train(\r\n  form = factor(Species) ~ .,\r\n  data = SVMTrain22,\r\n  #here we add classProbs because we want probs\r\n  trControl = trainControl(method = \"cv\", number = 10,\r\n                           classProbs =  TRUE),\r\n  method = \"svmPoly\",\r\n  preProcess = c(\"center\", \"scale\"),\r\n  tuneLength = 10)\r\n\r\niris_SVM22\r\n\r\n\r\nSupport Vector Machines with Polynomial Kernel \r\n\r\n90 samples\r\n 4 predictor\r\n 3 classes: 'setosa', 'versicolor', 'virginica' \r\n\r\nPre-processing: centered (4), scaled (4) \r\nResampling: Cross-Validated (10 fold) \r\nSummary of sample sizes: 81, 81, 81, 81, 81, 81, ... \r\nResampling results across tuning parameters:\r\n\r\n  degree  scale  C       Accuracy   Kappa    \r\n  1       1e-03    0.25  0.9111111  0.8666667\r\n  1       1e-03    0.50  0.9111111  0.8666667\r\n  1       1e-03    1.00  0.9111111  0.8666667\r\n  1       1e-03    2.00  0.9111111  0.8666667\r\n  1       1e-03    4.00  0.9111111  0.8666667\r\n  1       1e-03    8.00  0.9111111  0.8666667\r\n  1       1e-03   16.00  0.9333333  0.9000000\r\n  1       1e-03   32.00  0.9333333  0.9000000\r\n  1       1e-03   64.00  0.9555556  0.9333333\r\n  1       1e-03  128.00  0.9777778  0.9666667\r\n  1       1e-02    0.25  0.9111111  0.8666667\r\n  1       1e-02    0.50  0.9111111  0.8666667\r\n  1       1e-02    1.00  0.9111111  0.8666667\r\n  1       1e-02    2.00  0.9444444  0.9166667\r\n  1       1e-02    4.00  0.9444444  0.9166667\r\n  1       1e-02    8.00  0.9444444  0.9166667\r\n  1       1e-02   16.00  0.9777778  0.9666667\r\n  1       1e-02   32.00  0.9666667  0.9500000\r\n  1       1e-02   64.00  0.9666667  0.9500000\r\n  1       1e-02  128.00  0.9666667  0.9500000\r\n  1       1e-01    0.25  0.9444444  0.9166667\r\n  1       1e-01    0.50  0.9444444  0.9166667\r\n  1       1e-01    1.00  0.9555556  0.9333333\r\n  1       1e-01    2.00  0.9555556  0.9333333\r\n  1       1e-01    4.00  0.9666667  0.9500000\r\n  1       1e-01    8.00  0.9666667  0.9500000\r\n  1       1e-01   16.00  0.9666667  0.9500000\r\n  1       1e-01   32.00  0.9666667  0.9500000\r\n  1       1e-01   64.00  0.9666667  0.9500000\r\n  1       1e-01  128.00  0.9666667  0.9500000\r\n  1       1e+00    0.25  0.9555556  0.9333333\r\n  1       1e+00    0.50  0.9666667  0.9500000\r\n  1       1e+00    1.00  0.9666667  0.9500000\r\n  1       1e+00    2.00  0.9666667  0.9500000\r\n  1       1e+00    4.00  0.9666667  0.9500000\r\n  1       1e+00    8.00  0.9666667  0.9500000\r\n  1       1e+00   16.00  0.9777778  0.9666667\r\n  1       1e+00   32.00  0.9777778  0.9666667\r\n  1       1e+00   64.00  0.9666667  0.9500000\r\n  1       1e+00  128.00  0.9666667  0.9500000\r\n  1       1e+01    0.25  0.9666667  0.9500000\r\n  1       1e+01    0.50  0.9666667  0.9500000\r\n  1       1e+01    1.00  0.9777778  0.9666667\r\n  1       1e+01    2.00  0.9666667  0.9500000\r\n  1       1e+01    4.00  0.9666667  0.9500000\r\n  1       1e+01    8.00  0.9777778  0.9666667\r\n  1       1e+01   16.00  0.9666667  0.9500000\r\n  1       1e+01   32.00  0.9888889  0.9833333\r\n  1       1e+01   64.00  0.9888889  0.9833333\r\n  1       1e+01  128.00  0.9777778  0.9666667\r\n  1       1e+02    0.25  0.9888889  0.9833333\r\n  1       1e+02    0.50  0.9888889  0.9833333\r\n  1       1e+02    1.00  0.9888889  0.9833333\r\n  1       1e+02    2.00  0.9777778  0.9666667\r\n  1       1e+02    4.00  0.9888889  0.9833333\r\n  1       1e+02    8.00  0.9666667  0.9500000\r\n  1       1e+02   16.00  0.9666667  0.9500000\r\n  1       1e+02   32.00  0.9777778  0.9666667\r\n  1       1e+02   64.00  0.9666667  0.9500000\r\n  1       1e+02  128.00  0.9666667  0.9500000\r\n  1       1e+03    0.25  0.9777778  0.9666667\r\n  1       1e+03    0.50  0.9888889  0.9833333\r\n  1       1e+03    1.00  0.9777778  0.9666667\r\n  1       1e+03    2.00  0.9777778  0.9666667\r\n  1       1e+03    4.00  0.9777778  0.9666667\r\n  1       1e+03    8.00  0.9888889  0.9833333\r\n  1       1e+03   16.00  0.9888889  0.9833333\r\n  1       1e+03   32.00  0.9666667  0.9500000\r\n  1       1e+03   64.00  0.9888889  0.9833333\r\n  1       1e+03  128.00  0.9777778  0.9666667\r\n  1       1e+04    0.25  0.9666667  0.9500000\r\n  1       1e+04    0.50  0.9666667  0.9500000\r\n  1       1e+04    1.00  0.9888889  0.9833333\r\n  1       1e+04    2.00  0.9777778  0.9666667\r\n  1       1e+04    4.00  0.9666667  0.9500000\r\n  1       1e+04    8.00  0.9888889  0.9833333\r\n  1       1e+04   16.00  0.9777778  0.9666667\r\n  1       1e+04   32.00  0.9666667  0.9500000\r\n  1       1e+04   64.00  0.9888889  0.9833333\r\n  1       1e+04  128.00  0.9888889  0.9833333\r\n  1       1e+05    0.25  0.9666667  0.9500000\r\n  1       1e+05    0.50  0.9888889  0.9833333\r\n  1       1e+05    1.00  0.9666667  0.9500000\r\n  1       1e+05    2.00  0.9888889  0.9833333\r\n  1       1e+05    4.00  0.9666667  0.9500000\r\n  1       1e+05    8.00  0.9888889  0.9833333\r\n  1       1e+05   16.00  0.9666667  0.9500000\r\n  1       1e+05   32.00  0.9888889  0.9833333\r\n  1       1e+05   64.00  0.9666667  0.9500000\r\n  1       1e+05  128.00  0.9666667  0.9500000\r\n  1       1e+06    0.25  0.9777778  0.9666667\r\n  1       1e+06    0.50  0.9777778  0.9666667\r\n  1       1e+06    1.00  0.9888889  0.9833333\r\n  1       1e+06    2.00  0.9888889  0.9833333\r\n  1       1e+06    4.00  0.9666667  0.9500000\r\n  1       1e+06    8.00  0.9888889  0.9833333\r\n  1       1e+06   16.00  0.9666667  0.9500000\r\n  1       1e+06   32.00  0.9888889  0.9833333\r\n  1       1e+06   64.00  0.9888889  0.9833333\r\n  1       1e+06  128.00  0.9888889  0.9833333\r\n  2       1e-03    0.25  0.9111111  0.8666667\r\n  2       1e-03    0.50  0.9111111  0.8666667\r\n  2       1e-03    1.00  0.9111111  0.8666667\r\n  2       1e-03    2.00  0.9111111  0.8666667\r\n  2       1e-03    4.00  0.9111111  0.8666667\r\n  2       1e-03    8.00  0.9333333  0.9000000\r\n  2       1e-03   16.00  0.9333333  0.9000000\r\n  2       1e-03   32.00  0.9555556  0.9333333\r\n  2       1e-03   64.00  0.9777778  0.9666667\r\n  2       1e-03  128.00  0.9666667  0.9500000\r\n  2       1e-02    0.25  0.9111111  0.8666667\r\n  2       1e-02    0.50  0.9111111  0.8666667\r\n  2       1e-02    1.00  0.9444444  0.9166667\r\n  2       1e-02    2.00  0.9222222  0.8833333\r\n  2       1e-02    4.00  0.9666667  0.9500000\r\n  2       1e-02    8.00  0.9777778  0.9666667\r\n  2       1e-02   16.00  0.9666667  0.9500000\r\n  2       1e-02   32.00  0.9666667  0.9500000\r\n  2       1e-02   64.00  0.9666667  0.9500000\r\n  2       1e-02  128.00  0.9666667  0.9500000\r\n  2       1e-01    0.25  0.9444444  0.9166667\r\n  2       1e-01    0.50  0.9666667  0.9500000\r\n  2       1e-01    1.00  0.9666667  0.9500000\r\n  2       1e-01    2.00  0.9666667  0.9500000\r\n  2       1e-01    4.00  0.9666667  0.9500000\r\n  2       1e-01    8.00  0.9666667  0.9500000\r\n  2       1e-01   16.00  0.9666667  0.9500000\r\n  2       1e-01   32.00  0.9666667  0.9500000\r\n  2       1e-01   64.00  0.9888889  0.9833333\r\n  2       1e-01  128.00  0.9888889  0.9833333\r\n  2       1e+00    0.25  0.9666667  0.9500000\r\n  2       1e+00    0.50  0.9666667  0.9500000\r\n  2       1e+00    1.00  0.9666667  0.9500000\r\n  2       1e+00    2.00  0.9666667  0.9500000\r\n  2       1e+00    4.00  0.9777778  0.9666667\r\n  2       1e+00    8.00  0.9666667  0.9500000\r\n  2       1e+00   16.00  0.9888889  0.9833333\r\n  2       1e+00   32.00  0.9777778  0.9666667\r\n  2       1e+00   64.00  0.9888889  0.9833333\r\n  2       1e+00  128.00  0.9888889  0.9833333\r\n  2       1e+01    0.25  0.9777778  0.9666667\r\n  2       1e+01    0.50  0.9666667  0.9500000\r\n  2       1e+01    1.00  0.9777778  0.9666667\r\n  2       1e+01    2.00  0.9666667  0.9500000\r\n  2       1e+01    4.00  0.9555556  0.9333333\r\n  2       1e+01    8.00  0.9555556  0.9333333\r\n  2       1e+01   16.00  0.9777778  0.9666667\r\n  2       1e+01   32.00  0.9777778  0.9666667\r\n  2       1e+01   64.00  0.9666667  0.9500000\r\n  2       1e+01  128.00  0.9666667  0.9500000\r\n  2       1e+02    0.25  0.9666667  0.9500000\r\n  2       1e+02    0.50  0.9777778  0.9666667\r\n  2       1e+02    1.00  0.9777778  0.9666667\r\n  2       1e+02    2.00  0.9666667  0.9500000\r\n  2       1e+02    4.00  0.9777778  0.9666667\r\n  2       1e+02    8.00  0.9777778  0.9666667\r\n  2       1e+02   16.00  0.9777778  0.9666667\r\n  2       1e+02   32.00  0.9777778  0.9666667\r\n  2       1e+02   64.00  0.9555556  0.9333333\r\n  2       1e+02  128.00  0.9666667  0.9500000\r\n  2       1e+03    0.25  0.9555556  0.9333333\r\n  2       1e+03    0.50  0.9555556  0.9333333\r\n  2       1e+03    1.00  0.9666667  0.9500000\r\n  2       1e+03    2.00  0.9555556  0.9333333\r\n  2       1e+03    4.00  0.9555556  0.9333333\r\n  2       1e+03    8.00  0.9555556  0.9333333\r\n  2       1e+03   16.00  0.9555556  0.9333333\r\n  2       1e+03   32.00  0.9555556  0.9333333\r\n  2       1e+03   64.00  0.9555556  0.9333333\r\n  2       1e+03  128.00  0.9555556  0.9333333\r\n  2       1e+04    0.25  0.8888889  0.8333333\r\n  2       1e+04    0.50  0.9111111  0.8666667\r\n  2       1e+04    1.00  0.9222222  0.8833333\r\n  2       1e+04    2.00  0.9222222  0.8833333\r\n  2       1e+04    4.00  0.9000000  0.8500000\r\n  2       1e+04    8.00  0.9111111  0.8666667\r\n  2       1e+04   16.00  0.9222222  0.8833333\r\n  2       1e+04   32.00  0.8888889  0.8333333\r\n  2       1e+04   64.00  0.9111111  0.8666667\r\n  2       1e+04  128.00  0.9000000  0.8500000\r\n  2       1e+05    0.25  0.8888889  0.8333333\r\n  2       1e+05    0.50  0.9000000  0.8500000\r\n  2       1e+05    1.00  0.9111111  0.8666667\r\n  2       1e+05    2.00  0.8666667  0.8000000\r\n  2       1e+05    4.00  0.9111111  0.8666667\r\n  2       1e+05    8.00  0.8666667  0.8000000\r\n  2       1e+05   16.00  0.8888889  0.8333333\r\n  2       1e+05   32.00  0.9000000  0.8500000\r\n  2       1e+05   64.00  0.9000000  0.8500000\r\n  2       1e+05  128.00  0.8777778  0.8166667\r\n  2       1e+06    0.25  0.9000000  0.8500000\r\n  2       1e+06    0.50  0.9000000  0.8500000\r\n  2       1e+06    1.00  0.8888889  0.8333333\r\n  2       1e+06    2.00  0.8666667  0.8000000\r\n  2       1e+06    4.00  0.8777778  0.8166667\r\n  2       1e+06    8.00  0.9000000  0.8500000\r\n  2       1e+06   16.00  0.8888889  0.8333333\r\n  2       1e+06   32.00  0.8777778  0.8166667\r\n  2       1e+06   64.00  0.9000000  0.8500000\r\n  2       1e+06  128.00  0.8888889  0.8333333\r\n  3       1e-03    0.25  0.9111111  0.8666667\r\n  3       1e-03    0.50  0.9111111  0.8666667\r\n  3       1e-03    1.00  0.9111111  0.8666667\r\n  3       1e-03    2.00  0.9111111  0.8666667\r\n  3       1e-03    4.00  0.9111111  0.8666667\r\n  3       1e-03    8.00  0.9444444  0.9166667\r\n  3       1e-03   16.00  0.9444444  0.9166667\r\n  3       1e-03   32.00  0.9666667  0.9500000\r\n  3       1e-03   64.00  0.9666667  0.9500000\r\n  3       1e-03  128.00  0.9666667  0.9500000\r\n  3       1e-02    0.25  0.9111111  0.8666667\r\n  3       1e-02    0.50  0.9333333  0.9000000\r\n  3       1e-02    1.00  0.9222222  0.8833333\r\n  3       1e-02    2.00  0.9444444  0.9166667\r\n  3       1e-02    4.00  0.9777778  0.9666667\r\n  3       1e-02    8.00  0.9666667  0.9500000\r\n  3       1e-02   16.00  0.9666667  0.9500000\r\n  3       1e-02   32.00  0.9666667  0.9500000\r\n  3       1e-02   64.00  0.9666667  0.9500000\r\n  3       1e-02  128.00  0.9666667  0.9500000\r\n  3       1e-01    0.25  0.9777778  0.9666667\r\n  3       1e-01    0.50  0.9666667  0.9500000\r\n  3       1e-01    1.00  0.9666667  0.9500000\r\n  3       1e-01    2.00  0.9666667  0.9500000\r\n  3       1e-01    4.00  0.9666667  0.9500000\r\n  3       1e-01    8.00  0.9666667  0.9500000\r\n  3       1e-01   16.00  0.9666667  0.9500000\r\n  3       1e-01   32.00  0.9777778  0.9666667\r\n  3       1e-01   64.00  0.9666667  0.9500000\r\n  3       1e-01  128.00  0.9777778  0.9666667\r\n  3       1e+00    0.25  0.9777778  0.9666667\r\n  3       1e+00    0.50  0.9777778  0.9666667\r\n  3       1e+00    1.00  0.9777778  0.9666667\r\n  3       1e+00    2.00  0.9777778  0.9666667\r\n  3       1e+00    4.00  0.9777778  0.9666667\r\n  3       1e+00    8.00  0.9777778  0.9666667\r\n  3       1e+00   16.00  0.9777778  0.9666667\r\n  3       1e+00   32.00  0.9777778  0.9666667\r\n  3       1e+00   64.00  0.9777778  0.9666667\r\n  3       1e+00  128.00  0.9777778  0.9666667\r\n  3       1e+01    0.25  0.9666667  0.9500000\r\n  3       1e+01    0.50  0.9666667  0.9500000\r\n  3       1e+01    1.00  0.9666667  0.9500000\r\n  3       1e+01    2.00  0.9777778  0.9666667\r\n  3       1e+01    4.00  0.9555556  0.9333333\r\n  3       1e+01    8.00  0.9666667  0.9500000\r\n  3       1e+01   16.00  0.9666667  0.9500000\r\n  3       1e+01   32.00  0.9666667  0.9500000\r\n  3       1e+01   64.00  0.9666667  0.9500000\r\n  3       1e+01  128.00  0.9555556  0.9333333\r\n  3       1e+02    0.25  0.9555556  0.9333333\r\n  3       1e+02    0.50  0.9666667  0.9500000\r\n  3       1e+02    1.00  0.9666667  0.9500000\r\n  3       1e+02    2.00  0.9666667  0.9500000\r\n  3       1e+02    4.00  0.9666667  0.9500000\r\n  3       1e+02    8.00  0.9666667  0.9500000\r\n  3       1e+02   16.00  0.9666667  0.9500000\r\n  3       1e+02   32.00  0.9666667  0.9500000\r\n  3       1e+02   64.00  0.9666667  0.9500000\r\n  3       1e+02  128.00  0.9555556  0.9333333\r\n  3       1e+03    0.25  0.9555556  0.9333333\r\n  3       1e+03    0.50  0.9666667  0.9500000\r\n  3       1e+03    1.00  0.9555556  0.9333333\r\n  3       1e+03    2.00  0.9555556  0.9333333\r\n  3       1e+03    4.00  0.9666667  0.9500000\r\n  3       1e+03    8.00  0.9666667  0.9500000\r\n  3       1e+03   16.00  0.9666667  0.9500000\r\n  3       1e+03   32.00  0.9666667  0.9500000\r\n  3       1e+03   64.00  0.9666667  0.9500000\r\n  3       1e+03  128.00  0.9555556  0.9333333\r\n  3       1e+04    0.25  0.9666667  0.9500000\r\n  3       1e+04    0.50  0.9666667  0.9500000\r\n  3       1e+04    1.00  0.9666667  0.9500000\r\n  3       1e+04    2.00  0.9666667  0.9500000\r\n  3       1e+04    4.00  0.9666667  0.9500000\r\n  3       1e+04    8.00  0.9666667  0.9500000\r\n  3       1e+04   16.00  0.9666667  0.9500000\r\n  3       1e+04   32.00  0.9666667  0.9500000\r\n  3       1e+04   64.00  0.9666667  0.9500000\r\n  3       1e+04  128.00  0.9666667  0.9500000\r\n  3       1e+05    0.25  0.9666667  0.9500000\r\n  3       1e+05    0.50  0.9555556  0.9333333\r\n  3       1e+05    1.00  0.9666667  0.9500000\r\n  3       1e+05    2.00  0.9666667  0.9500000\r\n  3       1e+05    4.00  0.9666667  0.9500000\r\n  3       1e+05    8.00  0.9555556  0.9333333\r\n  3       1e+05   16.00  0.9666667  0.9500000\r\n  3       1e+05   32.00  0.9666667  0.9500000\r\n  3       1e+05   64.00  0.9666667  0.9500000\r\n  3       1e+05  128.00  0.9666667  0.9500000\r\n  3       1e+06    0.25  0.9666667  0.9500000\r\n  3       1e+06    0.50  0.9555556  0.9333333\r\n  3       1e+06    1.00  0.9666667  0.9500000\r\n  3       1e+06    2.00  0.9666667  0.9500000\r\n  3       1e+06    4.00  0.9666667  0.9500000\r\n  3       1e+06    8.00  0.9555556  0.9333333\r\n  3       1e+06   16.00  0.9666667  0.9500000\r\n  3       1e+06   32.00  0.9666667  0.9500000\r\n  3       1e+06   64.00  0.9666667  0.9500000\r\n  3       1e+06  128.00  0.9666667  0.9500000\r\n\r\nAccuracy was used to select the optimal model using the\r\n largest value.\r\nThe final values used for the model were degree = 1, scale = 100\r\n and C = 0.25.\r\n\r\n\r\n\r\n\r\n",
      "last_modified": "2021-12-10T17:33:05+09:00"
    }
  ],
  "collections": ["posts/posts.json"]
}
